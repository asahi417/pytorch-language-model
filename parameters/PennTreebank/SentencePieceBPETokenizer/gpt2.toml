vocab_size = 10000

n_layer = 12
n_embedding = 410
n_state_ffn = 2100
n_head = 10
n_context = 128
max_cache_size = 0
residual_dropout = 0.1
attention_dropout = 0.1
embedding_dropout = 0.1
initializer_range = 0.02

batch_size = 64

lr = 2.5e-4
weight_decay = 0.01

scheduler = 'linear'
warmup_steps = 0
total_steps = 30000

random_seed = 1234