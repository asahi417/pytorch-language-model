vocab_size = 10000

n_layer = 6
n_head = 8
n_positional_embedding = 512
n_embedding = 512
n_state_ffn = 2048
n_context = 128
n_context_memory = 128

dropout_residual = 0.1
dropout_attention = 0.0
dropout_embedding = 0.1

init_range = 0.001
batch_size = 22

lr = 0.00025
weight_decay = 1.2e-6
clip = 10.0
scheduler = 'constant'
warmup_steps = 0
total_steps = 12000
optimizer = 'adamw'
random_seed = 1111