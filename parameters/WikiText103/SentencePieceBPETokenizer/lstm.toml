vocab_size = 10000

batch_size = 512
embedding_dim = 256
n_hidden_units = 2048 # 1150
n_layers = 2 # 3
n_context = 100
tie_weights = true
init_range = 0.1

dropout_word = 0.1
dropout_embedding = 0.3
dropout_intermediate = 0.3
dropout_output = 0.1

scheduler = 'constant'
lr = 0.001
weight_decay = 0.0
total_steps = 30000

random_seed = 1234

clip = ''
pass_state = false
optimizer = 'adam'
