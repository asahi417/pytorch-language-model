n_layer = 12
n_embedding = 768
n_state_ffn = 3072
n_head = 12
n_context = 128
max_cache_size = 0
residual_dropout = 0.1
attention_dropout = 0.1
embedding_dropout = 0.1
vocab_size = 15432
initializer_range = 0.02

batch_size = 64

lr = 2.5e-4
weight_decay = 0.01

scheduler = 'linear'
warmup_steps = 2000
total_steps = 25000

random_seed = 1234