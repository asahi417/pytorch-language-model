vocab_size = 204

batch_size = 128
embedding_dim = 200
n_hidden_units = 700
n_layers = 7
n_context = 100
tie_weights = true
init_range = 0.1

dropout_word = 0.1
dropout_embedding = 0.65
dropout_intermediate = 0.65
dropout_output = 0.4

scheduler = 'constant'
lr = 0.001
weight_decay = 1.2e-6
total_steps = 400000

random_seed = 1234
pass_state = true

clip = ''

optimizer = 'adamw'
