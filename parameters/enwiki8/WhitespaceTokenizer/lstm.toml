vocab_size = 204

batch_size = 128
embedding_dim = 200
n_hidden_units = 700
n_layers = 7
n_context = 100
tie_weights = true
init_range = 0.1

dropout_word = 0.1
dropout_embedding = 0.65
dropout_intermediate = 0.65
dropout_output = 0.4

scheduler = 'constant'
lr = 0.0001
weight_decay = 1.2e-6
total_steps = 300000

random_seed = 1234

clip = 1

optimizer = 'sgd'
